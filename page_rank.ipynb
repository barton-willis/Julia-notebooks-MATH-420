{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google page rank\n",
    "__MATH 420__ <br>\n",
    "_Spring 2021_ <br>\n",
    "\n",
    "\n",
    "We would like a way to assign a numerical value, let's call it the _rank,_ that corresponds to the _popularity_ of a web page. We'll describe a method that is the basis of the Google page rank.\n",
    "\n",
    "To start our thinking about this, let's imagine that a popular page, say the _Wall Street Journal_ (WSJ), has a link to the _Kearney Hub._  The editor of the _Hub_ will be _thrilled_ with the traffic that might result from being linked from such a popular web page. But if the _Kearney Hub_ links to the _Wall Street Journal,_ I'd guess that the editors of the WSJ would barely notice. So our first insight is that the rank of a page depends on the ranks of the pages that link to it. If a highly ranked page links to the _Kearney Hub,_ for example, it raises the rank of the _Hub._  But if a lowly ranked page links to the _Hub,_ it doesn't affect the rank of the _Hub_ all that much. \n",
    "\n",
    "Our second insight is that if a page links to many pages, that diminishes the influence of a link. A visitor to a page that links to a million other pages, for example, might click on any one of a million links, but a visitor to a page that only links to just ten pages has a good chance of visiting one of these ten pages. So the more links a page has, the less influence it has on the ranks of the pages it links to. We can think of each link from a web page as a vote, with the weight of each vote as $1/n$, where $n$ is the number of links from a web page. Thus, the sum of all the votes from each web page is one. \n",
    "\n",
    "Given these insights, let's define the _rank_ of a page to equal the weighted sum of the ranks that link to it. Again, the weight of each link is the reciprocal of the number of pages it links to. So if a page links to $10$ other pages, the weight of each link is $1/10$.\n",
    "\n",
    "Let's take an example. Let's suppose we have four web pages labeled $A,B,C$, and $D$. \n",
    "\n",
    "Suppose pages $B$ and $C$ link to page $A$, and suppose page $B$ has a weight of $1/2$ (that is, it links to a total of two pages), and page $C$ has a weight of $1/3$ (thus page $C$ links to three pages). The rank of $A$ satisfies\n",
    "$$\n",
    "  \\text{rank}(A) = \\frac{1}{2}  \\text{rank} (B) + \\frac{1}{3}  \\text{rank}(C).\n",
    "$$\n",
    "\n",
    "For the rank of $B$, suppose that pages $A$ and $C$ link to page $B$. And suppose the weight of page $A$ is $1/2$. Then\n",
    "$$\n",
    "  \\text{rank}(B) = \\frac{1}{2}  \\text{rank} (A) + \\frac{1}{3} \\text{rank} (C).\n",
    "$$\n",
    "For the other two pages, let's suppose that \n",
    "$$\n",
    "  \\text{rank}(C) = \\frac{1}{2}  \\text{rank} (B) + \\text{rank} (D) ,\n",
    "$$\n",
    "$$\n",
    "\\text{rank}(D) = \\frac{1}{2}  \\text{rank} (A) +  \\frac{1}{3} \\text{rank} (C). \n",
    "$$\n",
    "\n",
    "In matrix notation, our equations are\n",
    "$$\n",
    "  \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix} = \\begin{bmatrix} 0 & 1/2 & 1/3 & 0 \\\\ 1/2 & 0 & 1/3 & 0 \\\\ 0 & 1/2 & 0 & 1 \\\\ 1/2 & 0 & 1/3 & 0 \\end{bmatrix} \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix}. \n",
    "$$\n",
    "Here I tired of writing $\\text{rank}(A)$, so I wrote $A$ instead; and similarly for the other variables.  We're expressed these linear equations in the form of a fixed point problem. Latter we'll use this fact as a way to find the ranks.\n",
    "\n",
    "The coefficient matrix has several nice properties: (a) every column sum is one and (b) all entries are in the interval $[0,1]$. Such a matrix is called a _Markov_ matrix. (See, for example, https://en.wikipedia.org/wiki/Stochastic_matrix). \n",
    "\n",
    "Alternatively, subtracting the left and right sides of this equation resuts in\n",
    "$$\n",
    "  \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = \n",
    "  \\begin{bmatrix} -1 & 1/2 & 1/3 & 0 \\\\ \n",
    "                   1/2 & -1 & 1/3 & 0 \\\\ \n",
    "                   0 & 1/2 & -1 & 1 \\\\ \n",
    "                   1/2 & 0 & 1/3 & -1 \\end{bmatrix} \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix}. \n",
    "$$\n",
    "The equations for the unknowns $A, B, C$, and $D$ are homogeneous and linear. Accordingly, any multiple of a solution provides another solution. This freedom allows us to require that the sum of the ranks have a specific value. Requiring that sum of the ranks be one,\n",
    "our linear equations are\n",
    "$$\n",
    "  \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = \n",
    "  \\begin{bmatrix} -1 & 1/2 & 1/3 & 0 \\\\ \n",
    "                   1/2 & -1 & 1/3 & 0 \\\\ \n",
    "                   0 & 1/2 & -1 & 1 \\\\ \n",
    "                   1/2 & 0 & 1/3 & -1 \\\\\n",
    "                   1 & 1 & 1 & 1  \\\\\n",
    "                   \\end{bmatrix} \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix}. \n",
    "$$\n",
    "These equations are _not_ homogeneous and they are _over determined_ (the number of unknowns is greater than the number of knowns).  It is _not_ certain that the equations have a solution, and if they do have a solution, it's possible that some of the ranks will be negative. Following the logic of how these equations were determined, a negative rank doesn't make much sense.\n",
    "\n",
    "The Peron-Forbenious theorem comes to the rescue. This theorem tells us that for any Markov matrix $M$, there is a vector $\\mathbf{x}$ such that $M \\mathbf{x} = \\mathbf{x}$\n",
    "and $1 = \\sum x_k$, where each $x_k$ is nonnegative.  More generally, if $\\mathbf{x}$ \n",
    "is a nonzero vector and $\\lambda$ is a number such that $M \\mathbf{x} = \\lambda \\mathbf{x}$, we say that $\\mathbf{x}$ is an eigenvector of the matrix $M$ with eigenvalue $\\lambda$.\n",
    "The Peron-Forbenious theorem tells us that is $M$ is a Markov matrix, then $M$ has at least one eigvector with eigenvalue one.\n",
    "\n",
    "\n",
    "Let's have Julia solve the eigenvalue problem for us. We'll need the package `LinearAlgebra`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the matrix `M` and the right-hand side `b` by hand. After that, we can \n",
    "use Julia's `\\` operator to solve the over determined linear system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [-1 1/2 1/3 0; 1/2 -1 1/3 0; 0 1/2 -1 1; 1/2 0 1/3 -1; 1 1 1 1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [0 ; 0 ; 0 ; 0 ; 1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.22222222222222204\n",
       " 0.22222222222222224\n",
       " 0.3333333333333334\n",
       " 0.22222222222222227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M \\ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, each solution is nonnegative, as the theory requires. For the theory, see https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem .\n",
    "\n",
    "As an alternative to solving the linear equations, let's use fixed point iteration to solve the equations. Here is a quickly written recursive method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed_point (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function fixed_point(M, x0, tol, iter=0)\n",
    "    if iter  < 100\n",
    "       x1 = M * x0\n",
    "       if norm(x1-x0, Inf) < tol x1 else fixed_point(M, x1, tol,iter+1) end\n",
    "    else\n",
    "        error(\"Fixed point sequence doesn't seem to converge\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it--we'll try an initial point of $[1,0,0,0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [0 1/2 1/3 0; 1/2 0 1/3 0; 0 1/2 0 1; 1/2 0 1/3 0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.22222228348255157\n",
       " 0.22222229093313217\n",
       " 0.3333331346511841\n",
       " 0.22222229093313217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(M, [1; 0 ; 0; 0], 1.0e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us try a different starting value. We get the same fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.22222232818603516\n",
       " 0.22222232818603516\n",
       " 0.33333301544189453\n",
       " 0.22222232818603516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(M, [1/4; 1/4 ; 1/4; 1/4], 1.0e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are familiar! This is exactly the result we got by solving the\n",
    "linear equations using the `\\` operator.  For our matrix, we can show that if the sum of the members of `x` is one, the sum of the members of $Mx$ is also one. Since started with a vector whose sum of components was one, the method returns a vector that also has a sum of components of one.\n",
    "\n",
    "We've described what Wikipedia (https://en.wikipedia.org/wiki/PageRank#Simplified_algorithm) refers to the _simplified version_.  \n",
    "\n",
    "The Patent for the Google Page Rank (https://patentimages.storage.googleapis.com/db/8f/cb/dad63e985797ec/US7058628.pdf) replaces the Markov matrix $M$ for the simplified version by \n",
    "$$\n",
    "  \\frac{\\alpha}{N} I  + (1 - \\alpha) M,\n",
    "$$\n",
    "where $N$ is the number of nodes, $I$ is an identity matrix, and $\\alpha \\in [0,1]$. In general, this is _not_ a Markov matrix, and its largest eigenvalue (called the _dominant eigenvalue_) is strictly less than one. Actually, all eigenvalues are inside the unit circle; consequently, it can be shown that _every_ fixed point sequence converges to the zero vector. And that would make the page rank of every page equal zero. Since every fixed point sequence converges to the zero vector when $\\alpha < 1$, generally $\\alpha$ is called a _damping factor._ \n",
    "\n",
    "But buried in the Patent application is \n",
    "\n",
    "\"_Note that in order to ensure convergence, the norm of p, must be made equal to 1  after each iteration_\"\n",
    "\n",
    "And this means that the original method modifies the fixed point sequence by dividing each term fixed point sequence by a norm (which norm, the one, two, or infinity, doesn't matter). This is known as the power method for finding the dominant eigenvalue (see https://en.wikipedia.org/wiki/Power_iteration).\n",
    "\n",
    "With or without a damping factor, the matrix used can have two or more linearly independent eigenvectors corresponding to the eigenvalue with the greatest magnitude. This happens, for example, when there are two or more nonempty disjoint sets of web pages (call them clusters) that are linked to other members of the subset, but not other clusters. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [0 1 0 0; 1 0 0 0; 0 0 0 1; 0 0 1 0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the starting value is $[1,0,0,0]$, we get an error because the fixed point sequence doesn't converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Fixed point sequence doesn't seem to converge",
     "output_type": "error",
     "traceback": [
      "Fixed point sequence doesn't seem to converge\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base .\\error.jl:35\n",
      " [2] fixed_point(M::Matrix{Int64}, x0::Vector{Int64}, tol::Float64, iter::Int64)\n",
      "   @ Main c:\\Users\\Barton\\Documents\\maxima-git\\Julia-notebooks-MATH-420\\page_rank.ipynb:6\n",
      " [3] fixed_point(M::Matrix{Int64}, x0::Vector{Int64}, tol::Float64, iter::Int64) (repeats 100 times)\n",
      "   @ Main c:\\Users\\Barton\\Documents\\maxima-git\\Julia-notebooks-MATH-420\\page_rank.ipynb:4\n",
      " [4] fixed_point(M::Matrix{Int64}, x0::Vector{Int64}, tol::Float64)\n",
      "   @ Main c:\\Users\\Barton\\Documents\\maxima-git\\Julia-notebooks-MATH-420\\page_rank.ipynb:2\n",
      " [5] top-level scope\n",
      "   @ c:\\Users\\Barton\\Documents\\maxima-git\\Julia-notebooks-MATH-420\\page_rank.ipynb:1"
     ]
    }
   ],
   "source": [
    "fixed_point(M, [1; 0 ; 0; 0], 1.0e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But changing to a starting value of $[1/4,1/4,1/4, 1/4]$, the fixed point sequence converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.25\n",
       " 0.25\n",
       " 0.25\n",
       " 0.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(M, [0.25; 0.25 ; 0.25; 0.25], 1.0e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling these pages $A$ though $D$, we see that $A$ and $B$ are linked and $C$ and $D$ are linked, but these two sets of nodes aren't linked together. What about the eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\n",
       "values:\n",
       "4-element Vector{Float64}:\n",
       " -0.9999999999999989\n",
       " -0.9999999999999989\n",
       "  1.0\n",
       "  1.0\n",
       "vectors:\n",
       "4×4 Matrix{Float64}:\n",
       "  0.707107   0.0       0.707107  0.0\n",
       " -0.707107   0.0       0.707107  0.0\n",
       "  0.0        0.707107  0.0       0.707107\n",
       "  0.0       -0.707107  0.0       0.707107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = eigen(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha! There are two eigenvectors with eigenvalue 1. Using one eigenvector, the rank of $A$ and $B$ tie, but the ranks of $C$ and $D$ are zero.  And the other eigenvector swaps this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " -0.9999999999999989\n",
       " -0.9999999999999989\n",
       "  1.0\n",
       "  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       "  0.707107   0.0       0.707107  0.0\n",
       " -0.707107   0.0       0.707107  0.0\n",
       "  0.0        0.707107  0.0       0.707107\n",
       "  0.0       -0.707107  0.0       0.707107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including a damping factor does still gives two linearly independent eigenvectors corresponding to the dominant eigenvalue.  \n",
    "\n",
    "One way to fix this is to have a fictitious ''super node'' that is linked to every page and every page is linked to the super node. Effectively, the super node idea then includes the possibility that a user will visit a page by entering a url instead of randomly clicking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google ranks, I suppose, tens of billions (maybe trillions?) or so of pages. Finding _all_ the eigenvalues of such a huge matrix isn't, I think, possible. And it isn't needed either. The iterative process can be done quickly to find the page rank. Reasonable estimates are that it takes Google a few weeks to construct the graph of links and a few days to compute the page ranks.\n",
    "\n",
    "The Google Page Rank is named partially in honor of Larry _Page,_ one of the co-founders of Google, not as you might guess after web _page._ But the idea of using eigenvalues to rank options was popularized by the mathematician Thomas Saaty _decades_ before Google used it to rank pages. Stigler's law of eponymy, says that \"states that no scientific discovery is named after its original discoverer.\" (https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy). And so it is with the Google Page Rank.\n",
    "\n",
    "The history of the concept of an eigenvalue goes back to at least Euler (1707 – 1783). About 230 years after Euler used eigenvectors and eigenvalues to describe the motion of ridged bodies, Larry Page used the same concept to launch one of the largest companies of all time.\n",
    "\n",
    "For more information, see https://en.wikipedia.org/wiki/PageRank ; and see https://en.wikipedia.org/wiki/Thomas_L._Saaty\n",
    "\n",
    "Here is an example that shows that including a damping factor does not alter the fact that $M$ has two linearly independent eigenvectors corresponding to the dominant eigenvalue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       " 0.0375  0.85    0.0     0.0\n",
       " 0.85    0.0375  0.0     0.0\n",
       " 0.0     0.0     0.0375  0.85\n",
       " 0.0     0.0     0.85    0.0375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = alpha/ N * I + (1-alpha) * M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\n",
       "values:\n",
       "4-element Vector{Float64}:\n",
       " -0.8124999999999989\n",
       " -0.8124999999999989\n",
       "  0.8875\n",
       "  0.8875\n",
       "vectors:\n",
       "4×4 Matrix{Float64}:\n",
       "  0.707107   0.0       0.707107  0.0\n",
       " -0.707107   0.0       0.707107  0.0\n",
       "  0.0        0.707107  0.0       0.707107\n",
       "  0.0       -0.707107  0.0       0.707107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eigen(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
