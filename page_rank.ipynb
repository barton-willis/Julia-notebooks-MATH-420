{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google page rank\n",
    "__MATH 420__ <br>\n",
    "_Spring 2021_ <br>\n",
    "\n",
    "\n",
    "We'll describe a method that is the basis of the Google page rank. Specifically, we would like a way to assign a numerical value, let's call it the _rank,_ that corresponds to the _popularity_ of a web page. \n",
    "\n",
    "To start our thinking about this, let's imagine that a popular webpage, say the _Wall Street Journal_ (WSJ), has a link to the _Kearney Hub._  The editor of the _Hub_ will be _thrilled_ with the traffic that might result from being linked from such a popular \n",
    "webpage. But if the _Kearney Hub_ links to the _Wall Street Journal,_ I'd guess that the editors of the WSJ would barely notice. So our first insight is that the rank of a page depends on the ranks of the pages that link to it. If a highly ranked page links to the _Kearney Hub,_ for example, it raises the rank of the _Hub._  But if a lowly ranked page links to the _Hub,_ it doesn't affect the rank of the _Hub_ all that much. \n",
    "\n",
    "Our second insight is that if a page links to many pages, that diminishes the influence of a link. A visitor to a page that links to a million other pages, for example, might click on any one of a million links, but a visitor to a page that only links to just ten pages has a better chance of visiting one of these ten pages. So the more links a page has, the less influence it has on the ranks of the pages it links to. We can think of each link from a web page as a vote, with the weight of each vote as $1/n$, where $n$ is the number of links from a web page. Thus, the sum of all the votes _from_ each web page is one. \n",
    "\n",
    "Given these insights, let's define the _rank_ of a page to equal to sum of the weights of the ranks that link to it. Again, the weight of each link is the reciprocal of the number of pages it links to. So if a page links to $10$ other pages, the weight of each link is $1/10$.\n",
    "\n",
    "Let's take an example. Let's suppose we have four web pages labeled $A,B,C$, and $D$. \n",
    "\n",
    "Suppose pages $B$ and $C$ link to page $A$, and suppose page $B$ has a weight of $1/2$ (that is, it links to a total of two pages), and page $C$ has a weight of $1/3$ (thus page $C$ links to three pages). The rank of $A$ satisfies\n",
    "$$\n",
    "  \\text{rank}(A) = \\frac{1}{2}  \\text{rank} (B) + \\frac{1}{3}  \\text{rank}(C).\n",
    "$$\n",
    "\n",
    "For the rank of $B$, suppose that pages $A$ and $C$ link to page $B$. And suppose the weight of page $A$ is $1/2$. Then\n",
    "$$\n",
    "  \\text{rank}(B) = \\frac{1}{2}  \\text{rank} (A) + \\frac{1}{3} \\text{rank} (C).\n",
    "$$\n",
    "For the other two pages, let's suppose that \n",
    "$$\n",
    "  \\text{rank}(C) = \\frac{1}{2}  \\text{rank} (B) + \\text{rank} (D) ,\n",
    "$$\n",
    "$$\n",
    "\\text{rank}(D) = \\frac{1}{2}  \\text{rank} (A) +  \\frac{1}{3} \\text{rank} (C). \n",
    "$$\n",
    "\n",
    "In matrix notation, our equations are\n",
    "$$\n",
    "  \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix} = \\begin{bmatrix} 0 & 1/2 & 1/3 & 0 \\\\ 1/2 & 0 & 1/3 & 0 \\\\ 0 & 1/2 & 0 & 1 \\\\ 1/2 & 0 & 1/3 & 0 \\end{bmatrix} \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix}. \n",
    "$$\n",
    "Here I tired of writing $\\text{rank}(A)$, so I wrote $A$ instead; and similarly for the other variables.  We have expressed these linear equations in the form of a fixed point problem. Latter we'll use this fact as a way to find the ranks.\n",
    "\n",
    "The coefficient matrix of our linear equation has several nice properties: \n",
    "\n",
    "(a) every column sum is one  <br>\n",
    "(b) all entries are in the interval $[0,1]$. \n",
    "\n",
    "These properties are _not_ a coincidence, but they are a consequence of the way that the matrix was constructed. Any square matrix with these two properties is called a _Markov_ matrix (https://en.wikipedia.org/wiki/Stochastic_matrix). \n",
    "\n",
    "We won't go into the theory, but the Markov property of this matrix implies that the page rank corresponds to the probabilty that clicking on a randomly selected link will land user on a webpage.\n",
    "\n",
    "Subtracting the left and right sides of these linear equations resuts in\n",
    "$$\n",
    "  \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = \n",
    "  \\begin{bmatrix} -1 & 1/2 & 1/3 & 0 \\\\ \n",
    "                   1/2 & -1 & 1/3 & 0 \\\\ \n",
    "                   0 & 1/2 & -1 & 1 \\\\ \n",
    "                   1/2 & 0 & 1/3 & -1 \\end{bmatrix} \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix}. \n",
    "$$\n",
    "The equations for the unknowns $A, B, C$, and $D$ are homogeneous and linear. Accordingly, any scalar multiple of a solution provides another solution. This freedom allows us to require that the sum of the ranks have a specific value. Requiring that sum of the ranks be one,\n",
    "our linear equations are\n",
    "$$\n",
    "  \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = \n",
    "  \\begin{bmatrix} -1 & 1/2 & 1/3 & 0 \\\\ \n",
    "                   1/2 & -1 & 1/3 & 0 \\\\ \n",
    "                   0 & 1/2 & -1 & 1 \\\\ \n",
    "                   1/2 & 0 & 1/3 & -1 \\\\\n",
    "                   1 & 1 & 1 & 1  \\\\\n",
    "                   \\end{bmatrix} \\begin{bmatrix} A \\\\ B \\\\ C \\\\ D \\end{bmatrix}. \n",
    "$$\n",
    "These equations are _not_ homogeneous and they are _over determined_ (the number of unknowns is greater than the number of knowns).  It is _not_ certain that the equations have a solution, and if they do have a solution, it's possible that some of the ranks will be negative. Following the logic of how these equations were determined, a negative rank doesn't make much sense.\n",
    "\n",
    "The Perron-Forbenious theorem comes to the rescue. This theorem tells us that for any Markov matrix $M$, there is a vector $\\mathbf{x}$ such that $M \\mathbf{x} = \\mathbf{x}$\n",
    "and $1 = \\sum x_k$, where each $x_k$ is nonnegative. But the solution isn't always unique.\n",
    "\n",
    "More generally, if $\\mathbf{x}$ is a nonzero vector and $\\lambda$ is a number such that $M \\mathbf{x} = \\lambda \\mathbf{x}$, we say that $\\mathbf{x}$ is an _eigenvector_ of the matrix $M$ with _eigenvalue_ $\\lambda$. In eigenvector/eigenvalue language, the Perron-Forbenious theorem tells us that if $M$ is a Markov matrix, the matrix $M$ has at least one eigvector with eigenvalue one.\n",
    "\n",
    "\n",
    "Let's have Julia solve the linear equations. To do this, we'll need the package `LinearAlgebra`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the matrix `M` and the right-hand side `b` by hand. After that, we can \n",
    "use Julia's `\\` operator to solve the over determined linear system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [-1 1/2 1/3 0; 1/2 -1 1/3 0; 0 1/2 -1 1; 1/2 0 1/3 -1; 1 1 1 1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [0 ; 0 ; 0 ; 0 ; 1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the theory tells us, every solution is nonnegative and the sum of the solutions is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.22222222222222204\n",
       " 0.22222222222222224\n",
       " 0.3333333333333334\n",
       " 0.22222222222222227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranks = M \\ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function `PR` that does these steps for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PR (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function PR(M::AbstractMatrix{T}) where T\n",
    "    n, m = size(M)\n",
    "    if n != m\n",
    "        throw(ArgumentError(\"The argument to `PR` must be square matrix.\"))\n",
    "    end\n",
    "    #  Subtract the identity matrix and append a row of ones\n",
    "    M = vcat(M - I, ones(T, 1, n))    \n",
    "    # Create the right-hand side vector b with zeros and set the last element to one\n",
    "    b = zeros(T, n+1)\n",
    "    b[end] = one(T)\n",
    "    # Solve the linear system M x = b and return the solution\n",
    "    M \\ b\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply `PR` to the problem we solved by hand. The coefficient matrix is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [0 1/2 1/3 0; 1/2 0 1/3 0; 0 1/2 0 1; 1/2 0 1/3 0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agrees with our previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.22222222222222204\n",
       " 0.22222222222222224\n",
       " 0.3333333333333334\n",
       " 0.22222222222222227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PR(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the idea that the ranks are the solution to a fixed point problem, an alterative way to solve the equations is to use fixed point iteration. When the input matrix is a Markov matrix and sum the members of the the initial term of the fixed \n",
    "sequence is one, this function will return a solution whose terms sum to one.\n",
    "\n",
    "Here is a simple function that does the iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed_point (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function fixed_point(M, x0, tol, iter=0)\n",
    "    if iter  < 100\n",
    "       x1 = M * x0\n",
    "       if norm(x1-x0, Inf) < tol x1 else fixed_point(M, x1, tol,iter+1) end\n",
    "    else\n",
    "        println(\"Fixed point sequence doesn't seem to converge.\")\n",
    "        nothing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it--we'll try an initial point of $[1,0,0,0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [0 1/2 1/3 0; 1/2 0 1/3 0; 0 1/2 0 1; 1/2 0 1/3 0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.22222228348255157\n",
       " 0.22222229093313217\n",
       " 0.3333331346511841\n",
       " 0.22222229093313217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(M, [1; 0 ; 0; 0], 1.0e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us try a different starting value. We get the same fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.22222232818603516\n",
       " 0.22222232818603516\n",
       " 0.33333301544189453\n",
       " 0.22222232818603516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(M, [1/4; 1/4 ; 1/4; 1/4], 1.0e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are familiar! This is exactly the result we got by solving the\n",
    "linear equations using the `\\` operator.  For our matrix, we can show that if the sum of the members of `x` is one, the sum of the members of $M x$ is also one. Since started with a vector whose sum of components was one, the method returns a vector that also has a sum of components of one.\n",
    "\n",
    "We've described what Wikipedia (https://en.wikipedia.org/wiki/PageRank#Simplified_algorithm) refers to the _simplified version_.  \n",
    "\n",
    "The Patent for the Google Page Rank (https://patentimages.storage.googleapis.com/db/8f/cb/dad63e985797ec/US7058628.pdf) replaces the Markov matrix $M$ for the simplified version by \n",
    "$$\n",
    "  \\frac{\\alpha}{N} I  + (1 - \\alpha) M,\n",
    "$$\n",
    "where $N$ is the number of nodes, $I$ is an identity matrix, and $\\alpha \\in [0,1]$ is called the _damping factor_. In general, this is _not_ a Markov matrix, and its largest eigenvalue (called the _dominant eigenvalue_) is strictly less than one. Actually, since all eigenvalues are inside the unit circle, it can be shown that _every_ fixed point sequence for the matrix $  \\frac{\\alpha}{N} I  + (1 - \\alpha) M$ converges to the zero vector. So how does this method work? Buried in the Patent application is \n",
    "\n",
    "\"_Note that in order to ensure convergence, the norm of p, must be made equal to 1  after each iteration_\"\n",
    "\n",
    "And this means that the original method modifies the fixed point sequence by dividing each term fixed point sequence by a norm (which norm, the one, two, or infinity, doesn't matter). This is known as the power method for finding the dominant eigenvalue (see https://en.wikipedia.org/wiki/Power_iteration).\n",
    "\n",
    "With or without a damping factor, the matrix used can have two or more linearly independent eigenvectors corresponding to the eigenvalue with the greatest magnitude. This happens, for example, when there are two or more nonempty disjoint sets of web pages (call them clusters) that are linked to other members of the subset, but not other clusters. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Int64}:\n",
       " 0  1  0  0\n",
       " 1  0  0  0\n",
       " 0  0  0  1\n",
       " 0  0  1  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = [0 1 0 0; 1 0 0 0; 0 0 0 1; 0 0 1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two column vectors $x$ with nonnegative members that are solutions to the linear system $M x = x$. These solutions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol1 = [1/2;1/2;0;0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M  * sol1 -  sol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol2 = [0;0;1/2;1/2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M  * sol2 -  sol2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every linear combination of these solutions is a solution; so another solution is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.25\n",
       " 0.25\n",
       " 0.25\n",
       " 0.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sol3 = (sol1 + sol2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M*sol3 - sol3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our linear equation method given by the function `PR` gives a four-way tie for first place. This corresponds to the solution `sol3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.25\n",
       " 0.2500000000000002\n",
       " 0.25000000000000006\n",
       " 0.25000000000000006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PR(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Julia's `\\` operator chose one solution from infinitely many solutions? The user documentaiton for `\\` says that the solution is the \n",
    "\n",
    "_minimum-norm least squares solution computed by a pivoted QR factorization of A and a rank estimate of A based on the R factor_.\n",
    "\n",
    "The meaning of this is beyond our class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the fixed point mathod gives us something else. When the starting value is \n",
    "$[1,0,0,0]$, we get an error because the fixed point sequence doesn't converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed point sequence doesn't seem to converge.\n"
     ]
    }
   ],
   "source": [
    "fixed_point(M, [1; 0 ; 0; 0], 1.0e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But changing to a starting value of $[1/4,1/4,1/4, 1/4]$, the fixed point sequence converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.25\n",
       " 0.25\n",
       " 0.25\n",
       " 0.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(M, [0.25; 0.25 ; 0.25; 0.25], 1.0e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way insure that the solution is unique is to introduce a fictitious ''super node'' that is linked to every page and every page is linked to the super node. Effectively, the super node idea includes the possibility that a user will visit a page by entering a url instead of randomly clicking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google ranks, I suppose, hundreds of billions (maybe trillions?) or so of pages. Solving the linear equations for the rank using row reduction for such a huge matrix isn't, I think, possible. And it isn't needed either. The iterative process can be done quickly to find the page rank. Reasonable estimates are that it takes Google a few weeks to construct the graph of links and a few days to compute the page ranks.\n",
    "\n",
    "The Google Page Rank is named partially in honor of Larry _Page,_ one of the co-founders of Google, not as you might guess after a web _page._ But the idea of using eigenvalues to rank options was popularized by the mathematician Thomas Saaty _decades_ before Google used it to rank pages. Stigler's law of eponymy, says that \"states that no scientific discovery is named after its original discoverer.\" (https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy). And so it is with the Google Page Rank. I'd say that the Patent Office _flubbed_ by issuing a patent for the Google Page Rank.\n",
    "\n",
    "The history of the concept of an eigenvalue goes back to at least Euler (1707 – 1783). About 230 years after Euler used eigenvectors and eigenvalues to describe the motion of ridged bodies, Larry Page used the same concept to launch one of the largest companies of all time.\n",
    "\n",
    "For more information, see https://en.wikipedia.org/wiki/PageRank ; and see https://en.wikipedia.org/wiki/Thomas_L._Saaty\n",
    "\n",
    "Here is an example that shows that using the Google Page Rank damping factor along with fixed point iteration makes the fixed point sequence converge to the zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.15;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a nonzero damping factor, the coefficient matrix is _not_ Markov. Since the modified matrix is not a Markov matrix, the probablistic interpertation of the matrix is _invalid_. Confusingly, the patent application uses probabilty language for method with the damping factor. Arguably, I'd say the patent application is a more of a bussines plan then it is  science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       " 0.0375  0.85    0.0     0.0\n",
       " 0.85    0.0375  0.0     0.0\n",
       " 0.0     0.0     0.0375  0.85\n",
       " 0.0     0.0     0.85    0.0375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = alpha/N * I + (1-alpha) * M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 7.559789073422259e-6\n",
       " 7.563896533750736e-6\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(xx, [1; 0 ; 0; 0], 1.0e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember that the patent application says that the norm of the vector must be adjusted to one after each iteration. Assuming this means dividing by the one norm after each iteration, a modified function for the fixed point sequence is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed_point (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function fixed_point(M, x0, tol, iter=0)\n",
    "    if iter  < 1000\n",
    "       x1 = M * x0\n",
    "       x1 = x1/norm(x1,1)\n",
    "       if norm(x1-x0, Inf) < tol x1 else fixed_point(M, x1, tol,iter+1) end\n",
    "    else\n",
    "        println(\"Fixed point sequence doesn't seem to converge.\")\n",
    "        nothing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the damping factor does _not_ create a unique fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.49999526002509537\n",
       " 0.5000047399749047\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(xx, [1; 0 ; 0; 0], 1.0e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.49999526002509537\n",
       " 0.5000047399749047"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_point(xx, [0; 0 ; 1; 0], 1.0e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's patent application says that in addition to the damping factor, it uses an intial \n",
    "term of the fixed point sequence with each member with the same value. I'm not sure of the theory behind the method, but apparently, with these modifications, the fixed point sequence converges to a unique soljution. My guess is that the damping factor has more to do with preventing overflow than with convergence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
